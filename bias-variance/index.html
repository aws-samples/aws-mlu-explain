<!DOCTYPE html><html><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><title>Bias Variance Tradeoff</title><meta name="description" content="MLU-Explain: Visual Explanation of the Bias Variance Tradeoff."><meta name="viewport" content="width=device-width, initial-scale=1"><meta property="og:image" content="https://mlu-explain.github.io/assets/ogimages/ogimage-bias-variance.png"><meta property="og:title" content="Bias Variance Tradeoff"><meta property="og:description" content="Learn the tradeoff between under- and over-fitting models, how it relates to bias and variance, and explore interactive examples related to LASSO and KNN."><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><link rel="icon" href="mlu_robot.5a492771.png"><link rel="stylesheet" href="katex.min.3c2484b0.css"><link rel="stylesheet" href="styles.565b223c.css"><script async src="https://www.googletagmanager.com/gtag/js?id=G-1FYW57GW3G"></script></head><body> <main> <div id="intro-icon"> <a href="https://mlu-explain.github.io"><svg width="50" height="50" viewBox="0 0 234 216" fill="none" xmlns="https://www.w3.org/2000/svg"> <g id="mlu_robot 1" clip-path="url(#clip0)"> <g> <path id="Vector" d="M90.6641 83.1836C96.8828 83.1836 101.941 78.1289 101.941 71.8906V71.8242C101.941 65.5898 96.8945 60.5312 90.6641 60.5312C84.4453 60.5312 79.3828 65.5898 79.3828 71.8242V71.8906C79.3828 78.1289 84.4336 83.1836 90.6641 83.1836Z" fill="#232f3e"></path> <path id="Vector_2" d="M143.305 83.1836C149.523 83.1836 154.586 78.1289 154.586 71.8906V71.8242C154.586 65.5898 149.535 60.5312 143.305 60.5312C137.09 60.5312 132.027 65.5898 132.027 71.8242V71.8906C132.027 78.1289 137.078 83.1836 143.305 83.1836Z" fill="#232f3e"></path> <path id="Vector_3" d="M163.586 159.402H173.609V122.641H163.586V159.402Z" fill="#232f3e"></path> <path id="Vector_4" d="M60.3594 159.402H70.3867V122.641H60.3594V159.402Z" fill="#232f3e"></path> <g id="Group"> <path id="Vector_5" d="M182.16 30.0781H51.8047V10.0234H182.16V30.0781ZM182.16 103.609H51.8047V40.1055H182.16V103.609ZM144.559 168.789H89.4062V113.641H144.559V168.789ZM0 0V10.0234H15.8789V46.7891H25.9023V10.0234H41.7812V113.641H79.3867V178.816H96.9297V215.578H106.957V178.816H127.016V215.578H137.039V178.816H154.586V113.641H192.188V10.0234H233.969V0" fill="#232f3e"></path> </g> </g> </g> <defs> <clipPath id="clip0"> <rect width="233.97" height="215.58" fill="#232f3e"></rect> </clipPath> </defs> </svg> <h2 class="logo">MLU-expl<span id="ai">AI</span>n</h2> </a> </div> <section id="intro">  <h1 id="intro__hed"> The <span id="bias">Bias</span> <span id="variance">Variance</span> Tradeoff </h1> <h3 id="intro__date"> <a href="https://twitter.com/jdwlbr">Jared Wilber</a> & Brent Werness, January 2021 </h3> <p id="intro__dek"> Prediction errors can be decomposed into two main subcomponents of interest: error from <span class="bolder">bias</span>, and error from <span class="bolder">variance</span>. The tradeoff between a model's ability to minimize bias and variance is foundational to training machine learning models, so it's worth taking the time to understand the concept. </p> </section>  <section id="scrolly"> <figure id="scroll-figure"> <div id="scroll-viz"></div> <div id="svg2"></div> </figure> <article> <div class="step" data-step="1"> <p class="step-head"></p> <p> Using our wildest imagination, we can picture a dataset consisting of features X and labels Y, as on the left. Also imagine that we’d like to generalize this relationship to additional values of X - that we’d like to predict future values based on what we’ve already seen before. <br><br> With our imagination now undoubtedly spent, we can take a very simple approach to modeling the relationship between X and Y by just drawing a line to the general trend of the data. </p> </div> <div class="step" data-step="2"> <p class="step-head">A Simple Model</p> <p> Our simple model isn’t the best at modeling the relationship - clearly there's information in the data that it's failing to capture. <br><br> We'll measure the performance of our model by looking at the <a href="https://en.wikipedia.org/wiki/Mean_squared_error">mean-squared error</a> of its output and the true values (displayed in the bottom barchart). Our model is close to some of the training points, but overall there's definitely room for improvement. <br><br> The error on the training data is important for model tuning, but what we really care about is how it performs on data we haven't seen before, called test data. So let's check that out as well. </p> </div> <div class="step" data-step="3"> <p class="step-head">Low Complexity & Underfitting</p> <p> Uh-oh, it looks like our earlier suspicions were correct - our model is garbage. The test error is even higher than the train error! <br><br> In this case, we say that our model is <span id="underfit-highlight">underfitting</span> the data: our model is so simple that it fails to adequately capture the relationships in the data. The high test error is a direct result of the lack of complexity of our model. <br><br> <span id="underfit-def">An underfit model is one that is too simple to accurately capture the relationships between its features X and label Y.</span> </p> </div> <div class="step" data-step="4"> <p class="step-head">A Complex Model</p> <p> Our previous model performed poorly because it was too simple. Let's try our luck with something more complex. In fact, let's get as complex as we can - let's train a model that predicts every point in our training data perfectly. <br><br> Great! Now our training error is zero. As the old saying goes in Tennessee: Fool me once - shame on you. Fool me twice - er... you can't get fooled again ;). </p> </div> <div class="step" data-step=" 5"> <p class="step-head">High Complexity & Overfitting</p> <p> Wait a second... Even though our training error from our model was effectively zero, the error on our test data is high. What gives? <br><br> Unsurprisingly, our model is too complicated. We say that it <span id="overfit-highlight">overfits</span> the data. Instead of learning the true trends underlying our dataset, it memorized noise and, as a result, the model is not generalizable to datasets beyond its training data. <br><br> <span id="overfit-def">Overfitting refers to the case when a model is so specific to the data on which it was trained that it is no longer applicable to different datasets.</span><br><br> In situations where your training error is low but your test error is high, you've likely overfit your model. </p> </div> <div class="step" data-step="6"> <p class="step-head">Test Error Decomposition</p> <p> Our test error can come as a result of both under- and over- fitting our data, but how do the two relate to each other? <br><br> In the general case, <span id="decomp-highlight">mean-squared error can be decomposed into three components: error due to bias, error due to variance, and error due to noise.</span> <br><br> <span class="katex-bv-text"></span> <br><br> Or, mathematically: <br><br> <span class="katex-bv-equation"></span> <br><br> We can’t do much about the irreducible<span class="katex-noise"></span> term, but we can make use of the relationship between both bias and variance to obtain better predictions. </p> </div> <div class="step" data-step="7"> <p class="step-head">Bias</p> <p> <span id="bias-def">Bias represents the difference between the average prediction and the true value</span>: <br><br> <span class="katex-bias"></span> <br><br> The term <span class="katex-bias-inline"></span> is a tricky one. It refers to the average prediction after the model has been trained over several independent datasets. We can think of the bias as measuring a <i>systematic</i> error in prediction. <br><br>These different model realizations are shown in the top chart, while the error decomposition (for each point of data) is shown in the bottom chart. <br><br> For underfit (low-complexity) models, the majority of our error comes from bias. </p> </div> <div class="step" data-step="8"> <p class="step-head">Variance</p> <p> As with bias, the notion of variance also relates to different realizations of our model. Specifically, <span id="variance-def">variance measures how much, on average, predictions vary for a given data point</span>: <br><br> <span class="katex-var"></span> <br><br> As you can see in the bottom plot, predictions from overfit (high-complexity) models show a lot more error from variance than from bias. It’s easy to imagine that any unseen data points will be predicted with high error. </p> </div> <div class="step" data-step="9"> <p class="step-head">Finding A Balance</p> <p> To obtain our best results, we should work to find a happy medium between a model that is so basic it fails to learn meaningful patterns in our data, and one that is so complex it fails to generalize to unseen data . <br><br> In other words, we don’t want an underfit model, but we don’t want an overfit model either. We want something in between - something with enough complexity to learn learn the generalizable patterns in our data. <br><br> <span id="balance-def">By trading some bias for variance (i.e. increasing the complexity of our model), and without going overboard, we can find a balanced model for our dataset</span>. </p> </div> <div class="step" data-step="10"> <p class="step-head">Across Complexities</p> <p> We just showed, at different levels of complexity, a sample of model realizations alongside their corresponding prediction error decompositions. <br><br> Let’s direct our focus to the error decompositions across model complexities. <br><br> For each level of complexity, we’ll aggregate the error decomposition across all data-points, and plot the aggregate errors at their level of complexity. <br><br> This aggregation applied to our balanced model (i.e. the middle level of complexity) is shown to the left. </p> </div> <div class="step" data-step="11"> <p class="step-head">The Bias Variance Trade-off</p> <p> Repeating this aggregation across our range of model complexities, we can see the relationship between bias and variance in prediction errors manifests itself as a U-shaped curve detailing the trade off between bias and variance. <br><br> When a model is too simple (i.e. small values along the x-axis), it ignores useful information, and the error is composed mostly of that from bias. <br><br> When a model is too complex (i.e. large values along the x-axis), it memorizes non-general patterns, and the error is composed mostly of that from variance. <br><br> <span id="conclusion">The ideal model aims to minimize both bias and variance. It lays in the sweet spot - not too simple, nor too complex. Achieving such a balance will yield the minimum error.</span> </p> </div> </article> </section>  <div class="model-text"> <br><br> <p> Many models have parameters that change the final learned models, called hyperparameters Let's look at how these hyperparameters may be used to control the the bias-variance tradeoff with two examples: LOESS Regression and K-Nearest Neighbors. </p> <br> </div> <section class="models"> <hr>  <div class="model-text-wrapper"> <h1 class="model-header">LOESS Regression</h1> <p class="model-text"> LOESS (LOcally Estimated Scatterplot Smoothing) regression is a nonparametric technique for fitting a smooth surface between an outcome and some predictor variables. The curve fitting at a given point is weighted by nearby data. This weighting is governed by a smoothing hyperparameter, which represents the proportion of neighboring data used to calculate each local fit. <br><br> Thus, the bias variance tradeoff for LOESS may be controlled for via the smoothness parameter. When the smoothness is small, the amount of data we consider is insufficient for an accurate fit, resulting in large variance. However, if we make the smoothness too high (i.e. over-smoothed), we trade local information for global, resulting in large bias. <br><br> Below a LOESS curve is fit to two variables. Randomize the training data to observe the effect different model realizations have on variance, and control the smoothness to observe the tradeoff between under- and over-fitting (and thus, bias and variance). </p> </div> <div class="model-controls"> <button class="data-button" id="button-loess"> Randomize Train Data </button> <div id="slider-container-loess"></div> </div> <div class="model-wrapper" style="text-align:center;justify-content:center"> <div id="fit-container-loess"></div> <div id="predict-container-loess"></div> </div>   <hr> </section> <section class="models">  <div class="model-text-wrapper"> <h1 class="model-header">K-Nearest Neighbors</h1> <p class="model-text"> K-Nearest Neighbors (KNN) classification is a simple technique for assigning class membership to a data point with some majority vote of its K-nearest neighbors. For example, when K = 1, the data point is simply assigned to the class of that single nearest neighbor. If K = 69, the data point is assigned to the majority class of its 69-nearest neighbors. <br><br> We can observe the bias variance tradeoff in KNN directly by playing with the hyperparameter K. When K is small, only a small number of neighbors are considered during the classification vote. The resulting islands and jagged boundaries are a result of high variance, as classifications are determined by very localized neighborhoods. For large values of K, we see very smoothed regions that deviate sharply from the true decision boundary - go too high and you’ll just obtain a majority vote. This is high bias. On the other hand, for medium values of K, we see smooth the regions that follow along the true decision boundary. <br><br> Explore the trade-off for yourself below! The plot on the left shows the training data. The plot on the right shows decision regions based on the current value of K. Deeper colors reflect more confidence in the classification. Hover over a point to see its classification to the right, and the K-nearest neighbors used for consideration to the left. </p> </div> <div class="model-controls"> <button class="data-button" id="button-knn">Randomize Data</button> <div id="slider-container"></div> </div> <div class="model-wrapper" style="text-align:center;justify-content:center"> <div id="fit-container"></div> <div id="predict-container"></div> </div>  </section> <hr> <section class="section-offset"> <div style="text-align:center"> <h1 class="model-header">What About Double Descent?</h1> <p class="model-text"> You may have heard about the phenomena, known as Double Descent, wherein which the classic U-shaped bias variance curve (seen above and in textbooks everywhere) is followed by a second dip (shown below). Such a phenomena must nullify the bias variance tradeoff we just spent so long explaining, right? </p> <br> <div id="dd-container"></div> <br> <p class="model-text"> Fret not, dear reader! As we will detail in our next series of articles, Double Descent actually supports the classical notion of the bias variance trade off. Stay tuned to learn more! <br><br> </p> </div> </section> <hr>  <section> <div style="text-align:center"> <h1 class="model-header">It's Finally Over</h1> <p class="model-text"> <i>Exhales deeply.</i> It's finally over. Thanks for reading! We hope that the article is insightful no matter where you are along your machine learning journey, and that you came away with a better undersatnding of the bias variance tradeoff. <br><br> To make things compact, we skipped over some relevant topics (such as regularization), but stay-tuned for more MLU-Explain articles, where we plan to explain those, and other, topics related to machine learning. <br><br> To learn more about machine learning, check out our <a href="https://aws.amazon.com/machine-learning/mlu/">self-paced courses</a>, our <a href="https://www.youtube.com/channel/UC12LqyqTQYbXatYS9AA7Nuw">youtube videos</a>, and the <a href="https://d2l.ai/">Dive into Deep Learning</a> textbook. If you have any comments/ideas/etc. related to MLU-Explain articles, feel free to <a href="https://twitter.com/jdwlbr"> reach out directly</a>. The code is available <a href="https://github.com/aws-samples/aws-mlu-explain">here</a>. </p> </div> </section> <hr> <section id="outro"> <br><br> <h1 class="model-header">References + Open Source</h1> <p class="model-text"> This article is a product of the following resources + the awesome people who made (& contributed to) them: </p> <br> <ul> <li> <a href="https://arxiv.org/abs/1812.11118">Reconciling modern machine learning practice and the bias-variance trade-off</a><br> (Mikhail Belkin; Daniel Hsu; Siyuan Ma; Soumik Mandal, 2019) </li> <li> <a href="https://web.stanford.edu/~hastie/Papers/ESLII.pdf"> The Elements of Statistical Learning</a> <br>(Trevor Hastie; Robert Tibshirani; Jerome Friedman, 2009). </li> <li> <a href="https://d2l.ai/"> Dive into Deep Learning</a> (Aston Zhang and Zachary C. Lipton and Mu Li and Alexander J. Smola, 2020). </li> <li> <a href="https://scott.fortmann-roe.com/docs/BiasVariance.html"> Understanding The Bias-Variance Tradeoff</a> (Scott Fortmann-Roe, 2012). </li> <li> <a href="https://www.statsdirect.com/help/nonparametric_methods/loess.htm"> LOESS Curve Fitting</a> (statsdirect.com). </li> <li> <a href="https://d3js.org/">D3.js</a> (Mike Bostock, Philippe Rivière) </li> <li> <a href="https://roughnotation.com/">Rough Notation</a> (Preet Shihn) </li> <li> <a href="https://katex.org/">KaTeX</a> (Emily Eisenberg, Sophie Alpert) </li> <li> <a href="https://github.com/russellgoldenberg/scrollama">Scrollama</a> (Russel Goldenberg) </li> <li> <a href="https://github.com/HarryStevens/d3-regression">d3-regression</a> (Harry Stevens) </li> </ul> <br><br> </section> </main> <script>function a(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],a("js",new Date),a("config","G-1FYW57GW3G");</script>  <script src="js.63b32ab2.js"></script> <script src="katexCalls.eb206bfb.js"></script> </body></html>
